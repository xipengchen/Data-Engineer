####################################################################################################################################
# Airflow module
from __future__ import print_function
from airflow.operators.python_operator import PythonOperator
#from airflow.operators.sensors import TimeDeltaSensor
from airflow.models import DAG
import logging

# Regular modules
import bson
import io 
import os
from pymongo import MongoClient
try:
    to_unicode = unicode
except NameError:
    to_unicode = str
    
import pandas as pd
import numpy as np
import zipfile as zf
from sqlalchemy import create_engine
import psycopg2
import re
from slugify import slugify
import shutil

# time module
import datetime
####################################################################################################################################
os.chdir('/home/ec2-user')    
####################################################################################################################################
def download_mongo_dump_normalize_to_s3(ds, **kwargs):
    # Task to download the latest Mongo DB dump data file and title file  
    
    import boto
    from boto.s3.key import Key    
    
    run_date = format(datetime.datetime.now().strftime('%Y-%m-%d'))  
    
    logging.info("Running for date : {}".format(run_date))
    
    # username and password for Amazon
    aws_access_key_id = kwargs['params']['aws_access_key_id']
    aws_access_key = kwargs['params']['aws_access_key']    
    s3_input_bucket = kwargs['params']['s3_input_bucket']   
    s3_output_bucket = kwargs['params']['s3_output_bucket']    

    key = boto.connect_s3(aws_access_key_id, aws_access_key)
    bucket = key.lookup(s3_input_bucket)    
            
    # # Download the latest mondo dump from S3
    
    # modified_date= [(file.last_modified, file) for file in bucket]
    # file_to_download = sorted(modified_date, cmp=lambda x,y: cmp(x[0], y[0]))[-1][1]
    
    os.chdir('/home/ec2-user/airflow_dags_results/title_links_feed/')
        
    # logging.info("Downloading MongoDB data dump")
    
    # file_to_download.get_contents_to_filename('mongo_dump.zip')

    # logging.info("Unzip MongoDB data dump")

    # # Unzip data dump
    # zip_handle = zf.ZipFile('mongo_dump.zip')
    # zip_handle.extractall()
    
    # # Load books
    # with open('edison-nye/books.bson','rb') as f:
    #      data = bson.decode_all(f.read()) 
        
    # # Load book collection
    # with open('edison-nye/newsletterbooks.bson','rb') as f:
    #     book = bson.decode_all(f.read())

    # Connect to production client and get data from db.collection
    
    logging.info("Connectting to the mongodb server")

    client_production = MongoClient('')
    
    logging.info("Getting data from edison-nye/books")
    collection_books = db.books
    data = list(collection_books.find())
    
    logging.info("Getting data from edison-nye/newsletterbooks")
    collection_newsletter = db.newsletterbooks
    book = list(collection_newsletter.find())

        
    def cleanhtml(raw_html):
      cleanr = re.compile('<.*?>')
      cleantext = re.sub(cleanr, '', raw_html)
      return cleantext        
    
    # Data reshaping
    result = []
    for item in data:
        dict={}
        ls=[]
        subdict={}
        dict['primary_isbn13']=item.get('primary_isbn')
        dict['bisac_status']=item.get('bisac_status')
        dict['pub_date']=item.get('pub_date')
        
        # get live_date
        dict['live_date']=item.get('onsaledate') if item.get('onsaledate') != None else None
        
        dict['series_name']=item.get('series_name')
        dict['volume']=int(item.get('volume')) if item.get('volume') is not None and item.get('volume') != '' else None
        
        dict['award_winner'] = item.get('award_winner')
        dict['us_list_price']=item.get('price')
        dict['image']=item.get('image')
        dict['format']=item.get('format')
        dict['asin']=item.get('asin')
        dict['apple_ean']=int(item.get('apple_ean')) if item.get('apple_ean') is not None else None
        dict['google_id']=item.get('google_id')
        dict['asin_active'] = 1
        if item.get('description') is not None:
            dict['description']=cleanhtml(item.get('description'))
        else:
            dict['description'] = item.get('description')
            
        if item.get('legacy_slugs') is not None:
            dict['legacy_slugs']=item.get('legacy_slugs')[0].encode('utf8')
        else:
            # If title doesn't have legacy slugs, then slug will be automated generated by slugifying the title name
            dict['legacy_slugs']=slugify(item.get('title')).encode('utf8') if item.get('title') != None else slugify('').encode('utf8')
        if item.get('partner_title') is not None:
            dict['partner_title']='Y'
        else:
           dict['partner_title']='N'
        dict['publisher']=item.get('publisher')
        dict['title']=item.get('title')
        dict['print'] = item.get('print')
        dict['territories'] = item.get('territories')
        
        if item.get('bundle') == 'Yes':
            dict['bundle'] = 'Y'
        else:
            dict['bundle'] = 'N'
        
        if item.get('marketingservices') == 'Yes':
             dict['marketingservices'] = 'Y'
        else:
            dict['marketingservices'] = 'N'
       
        dict['page_count'] = int(item.get('page_count')) if item.get('page_count') is not None and item.get('page_count') != '' else None
        
        if item.get('publicdomain') == 'Yes':
            dict['publicdomain']= 'Y'
        elif item.get('publicdomain') == 'No':
            dict['publicdomain']= 'N'
        else:
            dict['publicdomain']= None
            
        
        #Process retailer_site_links
        if item.get('retailer_site_links')is not None:
            try:
                link_list=item.get('retailer_site_links')['6']
            except:
                try:
                    link_list=item.get('retailer_site_links')['1']
                except:
                    link_list=item.get('retailer_site_links')
                logging.info('file check {}'.format(link_list))   
            #print(link_list)
            for item2 in link_list:
                if 'name' in item2:
                    subdict={}
                    subdict['retailer']=item2['name']
                    subdict['product_uri']=item2['url']
                    ls.append(subdict)
                else:
                    subdict={}
                    subdict['retailer']=np.nan
                    subdict['product_uri']=np.nan
                    ls.append(subdict)
            dict['retailer_site_links']=ls
        else:
            dict['retailer_site_links']=[{'retailer':'', 'product_uri':''}]

        dict['reviewquote']=item.get('reviewquote') if item.get('reviewquote') is not None and item.get('reviewquote') != '' else ''
        # if dict['reviewquote'] != None and dict['reviewquote'] != '':
        #     logging.info(dict['reviewquote'])

    	if item.get('libraryprice') == 'Yes':
            dict['libraryprice']= 'Y'
        elif item.get('libraryprice') == 'No':
            dict['libraryprice']= 'N'
        else: dict['libraryprice'] = item.get('libraryprice')

        result.append(dict)
        
    # Unnest json
    df = pd.io.json.json_normalize(result,record_path='retailer_site_links', 
                                   meta=['title','primary_isbn13','asin','apple_ean','google_id','description','publisher','partner_title','legacy_slugs','image', 'pub_date', 'live_date', 'series_name','volume','award_winner','us_list_price', 'bisac_status', 'asin_active', 'territories', 'marketingservices', 'publicdomain', 'page_count','bundle', 'print','libraryprice','reviewquote'])
    
    
    # Reorder columns
    df=df[['title', 'primary_isbn13','asin','apple_ean','google_id','publisher','territories', 'marketingservices', 'publicdomain', 'page_count','bundle', 'print','partner_title', 'bisac_status','pub_date','live_date','us_list_price','series_name','volume','award_winner','legacy_slugs', 'image','description', 'retailer', 'product_uri', 'asin_active','libraryprice','reviewquote']]
    
    logging.info('file check {}'.format(df[df['primary_isbn13']=='9780802148254'][['product_uri', 'retailer', 'title']]))
    # Process live_date with null value
    # df['live_date'] = df['live_date'].fillna('')
    
    #Don't worry about p2k/kep for partner titles
    df_partner = df.loc[df['partner_title']=='Y']
    df = df.loc[df['partner_title']=='N']
    
    #Load KEP asins and primaryISBN13s
    kep_df = pd.read_csv('KEP_TITLES.csv')
    #kep_df.head()
    kep_df.columns = ['isbn13', 'asin_kep']
    #Connect to mysql to get isbn13 mapping
    engine = create_engine('mysql+mysqlconnector://%s:%s@orim-internal-db01.cqbltkaqn0z7.us-east-1.rds.amazonaws.com/openroad_internal'%(mysql_user,mysql_password), echo=False)
    conn = engine.connect()
    sql = "select distinct isbn13,primary_isbn13 from ingests_firebrand_pricing;"
    firebrand = pd.read_sql(sql, conn)
    kep_df_firebrand = pd.merge(kep_df, firebrand, on=['isbn13'],how='left')
    kep_df_final =kep_df_firebrand.dropna()
    #Merge with mongo dump data to get other attributes
    kep = pd.merge(kep_df_final,df, on=['primary_isbn13'],how='left')
    kep.drop(['asin','isbn13'],axis = 1,inplace = True)
    kep['asin_active'] = 0
    kep.rename(columns={'asin_kep':'asin'}, inplace=True)
    # Reorder columns
    kep=kep[['title', 'primary_isbn13','asin','apple_ean','google_id','publisher','territories', 'marketingservices', 'publicdomain' ,'page_count','bundle', 'print','partner_title' ,'bisac_status','pub_date','live_date','us_list_price','series_name','volume','award_winner','legacy_slugs', 'image','description', 'retailer', 'product_uri', 'asin_active','libraryprice','reviewquote']]


    #Load non-KEP asins and primaryisbn13s
    #Connect to redshift
    sql = "SELECT distinct asin,reference_id FROM amazon_catalog WHERE activate = 1;"
    dbname = "sailthrudata"
    host = "sailthru-data.cmpnfzedptft.us-east-1.redshift.amazonaws.com"
    conn_string = "dbname=%s port='5439' user=%s password=%s host=%s" %(dbname, redshift_user, redshift_password, host)
        
    conn = psycopg2.connect(conn_string)
    cursor = conn.cursor()
    cursor.execute(sql)
    p2k_asins =cursor.fetchall()
    p2k_asins_df = pd.DataFrame(p2k_asins)
    p2k_asins_df.columns = ['asin_p2k', 'isbn13']
    #p2k_asins_df.to_csv('Test_P2k_asins.csv')
    p2k_df_firebrand = pd.merge(p2k_asins_df, firebrand, on=['isbn13'],how='left')
    p2k_df_final =p2k_df_firebrand.dropna()
    
    
    #Merge with mongo dump data to get other attributes
    p2k = pd.merge(df, p2k_df_final, on=['primary_isbn13'],how='left')
    #Use bson ASIN when ason from iobyte_amazon_catalog_feed is not available
    p2k["asin_p2k"][pd.isnull(p2k["asin_p2k"])] = p2k["asin"][pd.isnull(p2k["asin_p2k"])]
    p2k.drop(['asin','isbn13'],axis = 1,inplace = True)
    p2k['asin_active'] = 1
    p2k.rename(columns={'asin_p2k':'asin'}, inplace=True)
    # Reorder columns
    p2k=p2k[['title', 'primary_isbn13','asin','apple_ean','google_id','publisher','territories', 'marketingservices', 'publicdomain' ,'page_count','bundle', 'print','partner_title' ,'bisac_status','pub_date','live_date','us_list_price','series_name','volume','award_winner','legacy_slugs', 'image','description', 'retailer', 'product_uri', 'asin_active','libraryprice','reviewquote']]
    #df = p2k + kep
    
    frames = [p2k, kep]
    df1 = pd.concat(frames)
    frames = [df1,df_partner]
    df  = pd.concat(frames)
    
    # Get retailer links data of partner titles from newsletterbooks collection 
    data = book
    
    # Data reshaping
    result = []
    for item in data:
        dict={}
        ls=[]
        subdict={}
        dict['primary_isbn13']=item.get('primary_isbn')
        if item.get('retailer_links')is not None:
            link_list=item.get('retailer_links')['1']
            #print(link_list)
            for item2 in link_list:
                if 'name' in item2:
                    subdict={}
                    subdict['retailer']=item2['name']
                    subdict['product_uri']=item2['url']
                    ls.append(subdict)
            dict['retailer_site_links']=ls
        else:
            dict['retailer_site_links']=[{'retailer':'', 'product_uri':''}]
        result.append(dict)
    
    
    # Unnest json
    df_newsletter = pd.io.json.json_normalize(result,record_path='retailer_site_links', 
                                   meta=['primary_isbn13'])
    
    df_nonpartner = df.loc[df['partner_title']=='N']
    df_partner = df.loc[df['partner_title']=='Y']
    #df.loc[df['primary_isbn13']=='9781420141795']
    df_newsletter_partner = df_newsletter.loc[df_newsletter['primary_isbn13'].isin(df_partner['primary_isbn13'])]
    
    # Remove entries having empty product_uri
    df_newsletter_partner = df_newsletter_partner.loc[df_newsletter_partner['product_uri']!='']
    
    df_partner_new = df_partner.merge(df_newsletter_partner, on='primary_isbn13', how ='left')
    df_partner_new = df_partner_new.drop(['retailer_x', 'product_uri_x'], axis=1)
    df_partner_new = df_partner_new[['title','primary_isbn13','asin','apple_ean', 'google_id', 'publisher','territories', 'marketingservices', 'publicdomain','page_count','bundle', 'print','partner_title' ,'bisac_status','pub_date','live_date','us_list_price','series_name','volume','award_winner','legacy_slugs', 'image','description', 'retailer_y', 'product_uri_y', 'asin_active','libraryprice','reviewquote']]
    df_partner_new = df_partner_new.rename(columns={'retailer_y':'retailer', 'product_uri_y':'product_uri'})
    
    # Check if those non-partner having no urls have url in newsletter collection
    df_nonpartner_url = df_nonpartner.loc[df_nonpartner['product_uri']!='']
    
    df_nonpartner_nourl = df_nonpartner.loc[df_nonpartner['product_uri']=='']
    df_newsletter_nonpartner = df_newsletter.loc[df_newsletter['primary_isbn13'].isin(df_nonpartner_nourl['primary_isbn13'])]
    df_nonpartner_nourl_new = df_nonpartner_nourl.merge(df_newsletter_nonpartner, on = 'primary_isbn13', how = 'left')
    df_nonpartner_nourl_new = df_nonpartner_nourl_new.drop(['retailer_x', 'product_uri_x'], axis=1)
    df_nonpartner_nourl_new = df_nonpartner_nourl_new[['title','primary_isbn13','asin','apple_ean','google_id','publisher','territories', 'marketingservices', 'publicdomain','page_count','bundle', 'print','partner_title' ,'bisac_status','pub_date','live_date','us_list_price','series_name','volume','award_winner','legacy_slugs', 'image','description', 'retailer_y', 'product_uri_y','asin_active','libraryprice','reviewquote']]
    df_nonpartner_nourl_new = df_nonpartner_nourl_new.rename(columns={'retailer_y':'retailer', 'product_uri_y':'product_uri'})
    # Merge data frames
    df_final = df_nonpartner_url.append(df_partner_new, ignore_index=True)
    df_final = df_final.append(df_nonpartner_nourl_new, ignore_index=True)
    
    # Convert nan with empty string
    df_final.retailer = df_final.retailer.fillna('')
    df_final.product_uri = df_final.product_uri.fillna('')
    df_final.description = df_final.description.fillna('')
    df_final = df_final.drop_duplicates()
    
    # Rewrite the values of award_winner
    df_final['award_winner']= df_final['award_winner'].replace(True, 'Y')
    df_final['award_winner']= df_final['award_winner'].replace(False, 'N')
    df_final = df_final[['title','primary_isbn13','asin','apple_ean','google_id','publisher','territories', 'marketingservices', 'publicdomain','page_count','bundle', 'print','partner_title' ,'bisac_status','pub_date','live_date','us_list_price','series_name','volume','award_winner','legacy_slugs', 'image','description', 'retailer', 'product_uri','asin_active','libraryprice','reviewquote']]

    # Export as csv to load into MySQL table
    df_final.to_csv('title_links_feed.csv', encoding='utf-8', index = False)
    
    # Upload title data file to S3 bucket
    conn = boto.connect_s3(aws_access_key_id, aws_access_key)
    bucket = conn.get_bucket(s3_output_bucket, validate = False)
    #Upload to S3
    upload = Key(bucket)    
    
    #Save updated ebb list data to S3
    csv_buffer = io.BytesIO()
    df_final.to_csv(csv_buffer, index=False, encoding='utf8')
    csv_buffer.seek(0)
    
    #Save the complete ebb list data load file to S3
    upload.key = 'mongodb/title_links_feed.csv'
    
    logging.info("Uploading file - {} to S3".format(upload.key))
    
    upload.set_contents_from_string(csv_buffer.getvalue())  

    # logging.info("Removing the intermediate Mongo dump ")

    # # remove the mongo dump file
    # os.remove("/home/ec2-user/airflow_dags_results/title_links_feed/mongo_dump.zip")  
    
    # # remove the unzip file directory
    # shutil.rmtree("/home/ec2-user/airflow_dags_results/title_links_feed/edison-nye")

    logging.info("Task completed successfully ")


    
def title_data_to_redshift_and_mysql(ds, **kwargs):
    # Load the Redshift and MysSQL table with latest title data
   
    redshift_user = kwargs['params']['redshift_user']
    redshift_password = kwargs['params']['redshift_password']
              
    logging.info("Loading data to MySQL ")
    
    os.chdir('/home/ec2-user/airflow_dags_results/title_links_feed/')
    

    engine = create_engine('mysql+mysqlconnector://'+ mysql_user +':'+ mysql_password +'@orim-internal-db01.cqbltkaqn0z7.us-east-1.rds.amazonaws.com/openroad_internal', echo=False)
    conn = engine.connect()
    conn.execute('truncate table title_links_feed')
    
    df = pd.read_csv('/home/ec2-user/airflow_dags_results/title_links_feed/title_links_feed.csv')
    logging.info(df.shape)
    df.to_sql(name='title_links_feed', con=engine, if_exists='append', index=False, chunksize=1000)

    # mysql_table = 'title_links_feed'
    # mysql_staging = mysql_table + '_staging'
    # sql = 'DROP TABLE IF EXISTS {0}'.format(mysql_staging)
    # con.execute(sql)
    # sql = 'CREATE TABLE {0} LIKE {1};'.format(mysql_staging,mysql_table)
    # con.execute(sql)
    
    # df = pd.read_csv('/home/ec2-user/airflow_dags_results/title_links_feed/title_links_feed.csv')
    # logging.info(df.shape)
    # df.to_sql(name=mysql_staging, con=engine, if_exists='append', index=False, chunksize=1000)

    # sql = """DROP TABLE {}""".format(mysql_table)
    # con.execute(sql)
    # sql = """ALTER TABLE {0} RENAME TO {1}""".format(mysql_staging,mysql_table)
    # con.execute(sql)

    # # Creating a SQL engine for dumping data
    # engine = create_engine('mysql+mysqlconnector://'+ mysql_user +':'+ mysql_password +'@orim-internal-db01.cqbltkaqn0z7.us-east-1.rds.amazonaws.com/openroad_internal', echo=False)
    # conn = engine.connect()   

    # conn.execute('truncate table title_links_feed')
        
    # # Insert data
    # sql = """
    # LOAD DATA LOCAL INFILE 'title_links_feed.csv' 
    # INTO TABLE title_links_feed 
    # FIELDS TERMINATED BY ','
    # OPTIONALLY ENCLOSED BY '"'
    # lines terminated by '\n'
    # IGNORE 1 LINES (title, primary_isbn13, asin, apple_ean, google_id, publisher, territories, marketingservices, publicdomain ,page_count,bundle, print, partner_title, bisac_status, pub_date, live_date, us_list_price, series_name, volume, award_winner, legacy_slugs, image,
    # description, retailer, product_uri, asin_active);
    # """
    # conn.execute(sql)
    
    logging.info("MySQL table title_links_feed loaded successfully")
    
    logging.info("Loading data to Redshift ")
    
    # Connect to RedShift
    dbname = "sailthrudata"
    host = "sailthru-data.cmpnfzedptft.us-east-1.redshift.amazonaws.com"        
    conn_string = "dbname=%s port='5439' user=%s password=%s host=%s" %(dbname, redshift_user, redshift_password, host)
    conn = psycopg2.connect(conn_string)
    cursor = conn.cursor()

    # Delete existing data from table
    sql = "TRUNCATE table title_links_feed"
    cursor.execute(sql)
                                
    # Ingesting to redshift"
        
    logging.info("Ingesting all authors data to Redshift")    
    
    sql = """
            COPY title_links_feed 
            (title,primary_isbn13,asin,apple_ean,google_id,publisher,territories,marketingservices,publicdomain,page_count,bundle,print,partner_title,bisac_status,pub_date,live_date,us_list_price,series_name,volume,award_winner,legacy_slugs,image,description,retailer,product_uri,asin_active,libraryprice,reviewquote)
            FROM 's3://orim-misc-data/mongodb/title_links_feed.csv'
            CREDENTIALS 'aws_iam_role=arn:aws:iam::822605674378:role/DataPipelineRole'
            DELIMITER ','
            IGNOREHEADER 1
            EMPTYASNULL
            QUOTE '"'
            CSV
            REGION 'us-east-1';
         """
    cursor.execute(sql)
    conn.commit()
                                
    # # Ingesting to redshift"
    # redshift_table = 'title_links_feed'
    # redshift_staging = redshift_table + '_staging'
    # logging.info("Ingesting all authors data to Redshift")
    # cursor.execute('DROP TABLE IF EXISTS {0}'.format(redshift_staging))
    # cursor.execute('CREATE TABLE {0} LIKE {1};'.format(redshift_staging,redshift_table))

    # sql = """
    #         COPY {}
    #         (title,primary_isbn13,asin,apple_ean,google_id,publisher,territories,marketingservices,publicdomain,page_count,bundle,print,partner_title,bisac_status,pub_date,live_date,us_list_price,series_name,volume,award_winner,legacy_slugs,image,description,retailer,product_uri,asin_active,libraryprice,reviewquote)
    #         FROM 's3://orim-misc-data/mongodb/title_links_feed.csv'
    #         CREDENTIALS 'aws_iam_role=arn:aws:iam::822605674378:role/DataPipelineRole'
    #         DELIMITER ','
    #         IGNOREHEADER 1
    #         EMPTYASNULL
    #         QUOTE '"'
    #         CSV
    #         REGION 'us-east-1';
    #      """.format(redshift_staging)
    # cursor.execute(sql)
    # cursor.execute("DROP TABLE {};".format(redshift_table))
    # cursor.execute("ALTER TABLE {0} RENAME TO {1};".format(redshift_staging, redshift_table))
    # conn.commit()
    
    logging.info("Redshift table title_links_feed loaded successfully")

####################################################################################################################################
# Convert dag start date from local timezone to utc
dag_start_date_utc = datetime.datetime.strptime('2019-09-18', '%Y-%m-%d')

####################################################################################################################################
# Load env file
with open(os.environ['HOME']+'/env.txt') as f:
    env =dict([line.rstrip('\n').split('=') for line in f])
locals().update(env)

####################################################################################################################################

# Define DAG
args = {
        'owner': 'yliu',
        'start_date': dag_start_date_utc,
        'email': emaillist.split(','),
        'email_on_failure': True,
        'email_on_retry': True,
        'catchup': False
        }

dag = DAG(
        dag_id='title_links_feed_v3', 
        default_args=args,
        catchup=False,
        schedule_interval='30 10 * * *')



####################################################################################################################################
download_mongo_dump_normalize_to_s3 = PythonOperator(
        task_id = 'download_mongo_dump_normalize_to_s3',
        python_callable = download_mongo_dump_normalize_to_s3,
        provide_context=True,
        params={
                'aws_access_key_id': aws_access_key_id,
                'aws_access_key': aws_access_key,
                's3_input_bucket': 'orion-mongodb-backup',
                's3_output_bucket': 'orim-misc-data'
     			},        
        dag = dag)

title_data_to_redshift_and_mysql = PythonOperator(
        task_id = 'title_data_to_redshift_and_mysql',
        python_callable = title_data_to_redshift_and_mysql,
        provide_context=True,
        params={
                'redshift_user': redshift_user,
                'redshift_password': redshift_password,                
                's3_bucket': 'orim-misc-data'
     			},         
        dag = dag)

# Execute DAG
download_mongo_dump_normalize_to_s3 >>  title_data_to_redshift_and_mysql
